# League of Legends Analysis

by Zachary Hartman

## Introduction

League of Legends (LOL) is a popular online video game, where teams of five players play against each other. The dataset I will use has data from 2022 from professional matches. The dataset has many different statistics from individual data to team data.

In this project I will focus on team data. Specifically, I will use this team data to investigate my main question, how does this data allow us to make predictions about the outcome of matches.

The dataset has a total of 150,588 rows, which includes both teams and players. Of those rows, only 127,872 are complete. The only data I am interested in is the data for each team, so the number of rows is reduced to 25,098 rows, and 21,312 complete rows. The dataset also has 164 columns, but my investigation is only concerned with 10 of those columns, with 2 additional columns used only to study missingness. Here is an introduction to these rows:

- `result`: A column where 1 indicates that a team won, and 0 indicates that a team lost
- `totalgold`: Total gold generated by the team during the match
- `total_cs`: Total number of minions and monsters killed by the team during the match
- `goldat20`: The amount of gold generated by the team 20 minutes into the match
- `csat20`: The number of minions and monsters killed by the team 20 minutes into the match
- `killsat20`: The number of kills by the team 20 minutes into the match
- `xpat20`: The amount of experience gained by the team 20 minutes into the match
- `golddiffat20`: The difference in gold generated between the two teams 20 minutes into the match
- `csdiffat20`: The difference in the number of minions and monsters killed by the two teams 20 minutes into the match
- `xpdiffat20`: The difference in the amount of experience gained by the two teams 20 minutes into the match

The following columns will not be used in the hypothesis test or the prediction model, but will be used to test missingness.
- `firstbloodassist`: Whether or not the player assisted the first kill of the match
- `datacompleteness`: Whether or not the data for the match is complete. complete indicates the data is complete, and partial indicate the data is incomplete

## Data Cleaning and Exploratory Data Analysis

During the cleaning process, I keep only the rows corresponding to teams, as they contain all the data I am interested in. In addition to that, I only keep rows where the data is complete, which, as mentioned before, is still a large amount of data and more than enough to complete my analysis. I replace the column `result` with the column `win`, which represents the same information, but `win` represents the outcome of the game as a boolean,  where True indicates that a team won, and False indicates that a team lost. The outcome being represented as a boolean makes the information easier to see at a glance.

This is the head of my cleaned Dataframe:

| win   |   totalgold |   total_cs |   goldat20 |   csat20 |   killsat20 |   xpat20 |   golddiffat20 |   csdiffat20 |   xpdiffat20 |
|:------|------------:|-----------:|-----------:|---------:|------------:|---------:|---------------:|-------------:|-------------:|
| False |       47070 |        840 |      31962 |      631 |           5 |    36874 |           -944 |          -84 |        -4947 |
| True  |       52617 |        976 |      32906 |      715 |           7 |    41821 |            944 |           84 |         4947 |
| False |       57629 |       1209 |      31228 |      710 |           1 |    38596 |          -5140 |          -48 |        -3473 |
| True  |       71004 |       1257 |      36368 |      758 |           5 |    42069 |           5140 |           48 |         3473 |
| True  |       62868 |       1143 |      33717 |      752 |           3 |    41577 |           1744 |           14 |         2286 |

Note: `firstbloodassist` and `datacompleteness` are not included in this dataframe as they are not used in the hypothesis testing or prediction model, but will be used to study missingness.

I performed a univariate analysis on `total_cs`:

<iframe
  src="assets/minionkills.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>

The histogram shows the distribution of total minion and monster kills. The distribution peaks at the range 950-999, and the distribution is somewhat concentrated around that range.

I performed a bivariate analysis on `win` and `golddiffat20`:

<iframe
  src="assets/winvsgolddiff.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>

The box plots show the distribution of gold difference 20 minutes into the match for teams that won and lost. Teams that lost generally had a negative gold difference, while teams that won generally had a positive gold difference.

The following is an interesting aggregate from the data:

| win   |   goldat20 |   csat20 |   killsat20 |   xpat20 |
|:------|-----------:|---------:|------------:|---------:|
| False |    32699.4 |  659.932 |     4.94924 |  39485.8 |
| True  |    35705.5 |  687.538 |     7.99303 |  41512.3 |

I grouped the data based by the outcome of the game, and I looked at the mean for each of the columns.

## Assessment of Missingness

I believe that there are some columns that are not missing at random (NMAR). One such column that I believe is NMAR is `url`. I believe that when this column is missing, it is a result of the url for that match not existing or not being available. To make this column missing at random (MAR), I think that we could add a column that has a 1 if the url exist, and a 0 if the url doesn't exist.

As mentioned before, `firstbloodassist` and `datacompleteness` are going to be used test missingness. In my tests, if found that the missingness of `firstbloodassist` depended on the value in `datacompleteness` but didn't depend on the value in `win`. For both permuitation tests, I used difference in means as the test statistic, and I used 0.05 as the significance level for both as well.

First, I performed the permutation test on `firstbloodassist` and `datacompleteness`.

Null Hypothesis: The distribution of `datacompleteness` when `firstbloodassist` is missing is the same as when it is not missing

Alternative Hypothesis: The distribution of `datacompletenesss` when `firstbloodassist` is missing is different as when it is not missing

After performing the test, the observed test statistic was 0.83 and the p-value was 0.0. Below is the empirical distribution for the difference in means:

<iframe
  src="assets/missing1.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>

The p-value of 0.0 is less than 0.05, so we reject the null hypothesis and conclude that the missingness of `firstbloodassist` depends on `datacompleteness`.

The second permutation test I did was on `firstbloodassist` and `win`.

Null Hypothesis: The distribution of `result` when `firstbloodassist` is missing is the same as when it is not missing

Alternative Hypothesis: The distribution of `result` when `firstbloodassist` is missing is different as when it is not missing

After performing the test, the observed test statistic was 0.0 and the p-value was 1.0. Below is the empirical distribution for the difference in means:

<iframe
  src="assets/missing2.html"
  width="800"
  height="600"
  frameborder="0"
></iframe>

The p-value of 1.0 is greater than 0.05, so we accept the null hypohtesis and conclude that the missingness of `firstbloodassist` does not depend on `result`.

## Hypothesis Testing

In the hypothesis test, I performed a permutation test to determine whether there is a difference in `total_cs`, the number of minions and monsters killed, between the winning and losing teams.

Null Hypothesis: The number of minions and monsters killed by winning and losing teams have the same distribution

Alternative Hypothesis: The winning teams kill more minions and monsters than losing teams

I  used a test statistic of difference in means, and I used a significance level of 0.05.

As a result of my testing, I found an observed test statistic of 56.06, and a p-value of 0.0. As the p-value of 0.0 is less than 0.05, we reject the null hypothesis and conclude that the difference in the number of minions and monsters killed by winning and losing teams is statistically significant.

## Framing a Prediction Problem

Following the main question of my project, I want to be able to predict the outcome of a game during the game, that is, whether or not a team will win or lose. This type of prediction problem will be accomplished using binary classification, as the team will either win or lose. I want to predict the outcome of the match before it is over, so I will choose to use data collected 20 minutes into the match. I chose 20 minutes as I wanted a time somewhere in the middle of the match, but also more towards the end, so there will be more data to inform my predictions.

The variable I will be predicting is the value in the `win` column. I will be using accuracy as my metric to evaluate my model. I chose accuracy over other metrics because I want the model to be correct as much of the time as possible, and the distribution of false positives and false negatives is not as important to me. At the time of the prediction, I will have access to data collected at or before the 20 minute mark in the game. As such, I will use the data collected at 20 minutes, as it provides me the most data available at that time.

## Baseline Model

For the baseline model I used a Random Forest Classifier at `max_depth`=5. The features I used were `goldat20`, `csat20`, `killsat20`, and `xpat20`. All of these features were quantitative and no encodings were necessary. 

The resulting model had an accuracy of 0.75, or 75%. I think this is good performance for my model, as it is able to predict the correct outcome 75% of the time. As the model only uses data available at 20 minutes into the match, I think 75% accuracy is good as it would be very difficult to have near perfect accuracy when there is so much time left to play in the match.

## Final Model

In making my final model, I added three more features: `golddiffat20`, `csdiffat20`, and `xpdiffat20`. I thought that these would be good features to add because they provide information about how the two teams playing each other are doing compared to each other. This adds more information and allows the model to consider how far ahead a team might be, rather than just looking at values with no indication as to how the opponent is doing.

I continued with a Random Forest Classifier, but I ran a GridSearchCV to determine the optimal `max_depth` for the model. I ran it, testing values from 5 to 200, with a step of 5 between each, which resulted in a `max_depth` of 10 being the best for performance. The accuracy of the model improved a bit, up to 0.785, or 78.5%. I think that this improvement is actually larger than it looks, because the prediction is not being made of data from the full game, just from the first 20 minutes, and the model can't predict unexpected changes that may happen after 20 minutes, so I think the final model's performance is very good.

## Fairness Analysis

I chose to test my model's fairness on teams with an absolute gold difference of less than 1,000. I performed a permutation test to determine whether or not my model was fair. 

Null Hypothesis: The model is fair, the accuracy for both groups are roughly the same

Alternative Hypothesis: The model is unfair, accuracy for teams with an absolute gold difference under 1,000 is less than teams with an absolute gold difference above 10,00

I used a test statistic of difference in means, with a significance level of 0.05.

After performing the test, I found an observed test statistice of 0.116, and a p-value of 0.0. As the p-value of 0.0 is less than 0.05, we reject the null and conclude that teams with an absolute gold difference under 1,000 are less accurately predicted.
